# -*- coding: utf-8 -*-
"""mainfyp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kRRcK9CpBgKMV3gZeMIHsmurkNR4QPSV
"""

import streamlit as st
import numpy as np
import tensorflow as tf
from PIL import Image
import io
import requests
import json
import os
import tempfile
import librosa

# Set page configuration
st.set_page_config(
    page_title="Speech Emotion Recognition",
    page_icon="üé≠",
    layout="centered"
)

# Load model with caching
@st.cache_resource
def load_model():
    model_path = "./res_model.keras"
    if os.path.exists(model_path):
        print(f"Loading model from {model_path}")
        return tf.keras.models.load_model(model_path)
    else:
        st.error(f"Model file not found at {model_path}. Please run fyp_test.py first.")
        return None

# Generate emoticon using OpenAI API
def generate_emoticon(emotion):
    try:
        api_url = "https://api.openai.com/v1/images/generations"
        api_key = st.secrets.get("FYP_API_KEY")
        if not api_key:
            st.warning("OpenAI API key not configured. Using emoji fallback.")
            return None

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        prompt_mapping = {
            'happy': "a simple happy emoticon with a big smile",
            'sad': "a simple sad emoticon with a frown and tear",
            'angry': "a simple angry emoticon with furrowed brows",
            'neutral': "a simple neutral emoticon with straight face",
            'fear': "a simple scared emoticon with wide eyes",
            'disgust': "a simple disgusted emoticon with wrinkled nose",
            'surprise': "a simple surprised emoticon with open mouth"
        }

        prompt = prompt_mapping.get(emotion, "a simple emoticon")

        payload = {
            "prompt": prompt,
            "n": 1,
            "size": "256x256"
        }

        with st.spinner('Generating emoticon...'):
            response = requests.post(api_url, headers=headers, data=json.dumps(payload))
            response_data = response.json()

            if 'error' in response_data:
                st.error(f"API Error: {response_data['error']['message']}")
                return None

            image_url = response_data['data'][0]['url']
            image_response = requests.get(image_url)
            if image_response.status_code == 200:
                return image_response.content
            else:
                st.error(f"Failed to download image: {image_response.status_code}")
                return None

    except Exception as e:
        st.error(f"Error generating emoticon: {e}")
        return None

# Your original augmentation functions (add_noise, pitching, stretching) should be defined or imported here
# For example:
def add_noise(data, random=True):
    # Simple noise addition example
    noise_amp = 0.005 * np.random.uniform() if random else 0.005
    noise = noise_amp * np.random.normal(size=data.shape[0])
    return data + noise

def pitching(data, sr, random=True):
    pitch_factor = np.random.uniform(-2, 2) if random else 0
    return librosa.effects.pitch_shift(data, sr, n_steps=pitch_factor)

def stretching(data, rate=1.0):
    return librosa.effects.time_stretch(data, rate)

# Your original extract_features function
def extract_features(data, sr, frame_length=2048, hop_length=512):
    result = np.array([])
    mfcc_feat = librosa.feature.mfcc(y=data, sr=sr, n_mfcc=13, hop_length=hop_length, n_fft=frame_length)
    for i in range(mfcc_feat.shape[0]):
        result = np.hstack((result,
                           np.mean(mfcc_feat[i]),
                           np.std(mfcc_feat[i]),
                           np.min(mfcc_feat[i]),
                           np.max(mfcc_feat[i])))
    return result

# Your original get_features function with augmentations
def get_features(path, duration=2.5, offset=0.6):
    data, sr = librosa.load(path, duration=duration, offset=offset)
    target_length = int(sr * duration)
    if len(data) < target_length:
        data = np.pad(data, (0, target_length - len(data)))
    elif len(data) > target_length:
        data = data[:target_length]

    aud = extract_features(data, sr)
    audio = np.array([aud])

    noised_audio = add_noise(data, random=True)
    aud2 = extract_features(noised_audio, sr)
    audio = np.vstack((audio, [aud2]))

    pitched_audio = pitching(data, sr, random=True)
    aud3 = extract_features(pitched_audio, sr)
    audio = np.vstack((audio, [aud3]))

    pitched_audio1 = pitching(data, sr, random=True)
    pitched_noised_audio = add_noise(pitched_audio1, random=True)
    aud4 = extract_features(pitched_noised_audio, sr)
    audio = np.vstack((audio, [aud4]))

    stretch_rate = 0.8 + np.random.random() * 0.4
    stretched_audio = stretching(data, rate=stretch_rate)
    if len(stretched_audio) < target_length:
        stretched_audio = np.pad(stretched_audio, (0, target_length - len(stretched_audio)))
    elif len(stretched_audio) > target_length:
        stretched_audio = stretched_audio[:target_length]
    aud5 = extract_features(stretched_audio, sr)
    audio = np.vstack((audio, [aud5]))

    stretched_audio2 = stretching(data, rate=stretch_rate)
    if len(stretched_audio2) < target_length:
        stretched_audio2 = np.pad(stretched_audio2, (0, target_length - len(stretched_audio2)))
    elif len(stretched_audio2) > target_length:
        stretched_audio2 = stretched_audio2[:target_length]
    stretched_noised_audio = add_noise(stretched_audio2, random=True)
    aud6 = extract_features(stretched_noised_audio, sr)
    audio = np.vstack((audio, [aud6]))

    return audio

def process_features(features, model):
    # Reshape features to (batch_size, timesteps, 1)
    if len(features.shape) == 1:
        features = features.reshape(1, -1, 1)
    elif len(features.shape) == 2:
        features = np.expand_dims(features, axis=-1)

    with st.spinner('Analyzing emotion...'):
        predictions = model.predict(features, verbose=0)
        emotion_index = np.argmax(predictions[0])
        emotion_labels = ['neutral', 'surprise', 'happy', 'fear', 'sad', 'angry', 'disgust']
        emotion = emotion_labels[emotion_index]
        confidence = float(predictions[0][emotion_index])

    col1, col2 = st.columns(2)
    with col1:
        st.subheader("Detected Emotion")
        emotion_colors = {
            'happy': '#FFD700', 'sad': '#4169E1', 'angry': '#DC143C',
            'neutral': '#808080', 'fear': '#9932CC', 'disgust': '#228B22', 'surprise': '#FF8C00'
        }
        st.markdown(
            f"""
            <div style="background-color: {emotion_colors.get(emotion, '#808080')};
                      padding: 20px; border-radius: 10px; text-align: center;
                      color: white; font-size: 24px; font-weight: bold;">
                {emotion.upper()}
            </div>
            """, unsafe_allow_html=True)
        st.subheader("Confidence")
        st.progress(confidence)
        st.text(f"{confidence:.2%}")
        st.subheader("All Probabilities")
        for i, label in enumerate(emotion_labels):
            st.text(f"{label.capitalize()}: {predictions[0][i]:.4f}")

    with col2:
        st.subheader("Emoticon")
        emoticon_data = generate_emoticon(emotion)
        if emoticon_data:
            image = Image.open(io.BytesIO(emoticon_data))
            st.image(image, caption=f"{emotion.capitalize()} Emoticon", use_column_width=True)
        else:
            emotion_emojis = {
                'happy': "üòä", 'sad': "üò¢", 'angry': "üò†",
                'neutral': "üòê", 'fear': "üò®", 'disgust': "ü§¢", 'surprise': "üò≤"
            }
            st.markdown(f"<h1 style='text-align: center; font-size: 100px;'>{emotion_emojis.get(emotion, '‚ùì')}</h1>", unsafe_allow_html=True)

def main():
    st.title("üé≠ Speech Emotion Recognition")
    st.write("Upload an MP4 audio file to detect emotion")

    with st.sidebar:
        st.header("About")
        st.info("""
        This app recognizes emotions from speech using a neural network.

        Supported emotions:
        - Happy
        - Sad
        - Angry
        - Neutral
        - Fear
        - Disgust
        - Surprise
        """)
        st.header("Instructions")
        st.write("""
        1. Upload a WAV audio file containing speech
        2. The app will process your audio and predict the emotion
        """)

    model = load_model()
    if model is None:
        st.warning("Please run fyp_test.py to create the model before using this app.")
        return

    uploaded_file = st.file_uploader("Upload your audio file", type=["wav])

    if uploaded_file is not None:
        st.audio(uploaded_file, format='audio/wav')

        with tempfile.NamedTemporaryFile(suffix=".wav") as temp_audio:
            temp_audio.write(uploaded_file.read())
            temp_audio.flush()

            features = get_features(temp_audio.name)
            process_features(features, model)

if __name__ == "__main__":
    main()
