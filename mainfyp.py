# -*- coding: utf-8 -*-
"""mainfyp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kRRcK9CpBgKMV3gZeMIHsmurkNR4QPSV
"""


import streamlit as st
import numpy as np
import tensorflow as tf
from PIL import Image
import io
import requests
import json
import os
import tempfile
from pydub import AudioSegment

# Your existing feature extraction functions assumed imported here:
# from your_feature_module import get_features, process_features

# Set page configuration
st.set_page_config(
    page_title="Speech Emotion Recognition",
    page_icon="üé≠",
    layout="centered"
)

@st.cache_resource
def load_model():
    model_path = "./res_model.keras"
    if os.path.exists(model_path):
        return tf.keras.models.load_model(model_path)
    else:
        st.error(f"Model file not found at {model_path}. Please run fyp_test.py first.")
        return None
def process_features(features, model):
    # Ensure features have shape (batch_size, timesteps, features)
    if len(features.shape) == 1:
        # Single sample, shape (52,)
        features = features.reshape(1, -1, 1)
    elif len(features.shape) == 2:
        # Multiple samples, shape (batch_size, 52)
        features = np.expand_dims(features, axis=-1)

    predictions = model.predict(features, verbose=0)
def generate_emoticon(emotion):
    try:
        api_url = "https://api.openai.com/v1/images/generations"
        api_key = st.secrets.get("FYP_API_KEY")
        if not api_key:
            st.warning("OpenAI API key not configured. Using emoji fallback.")
            return None

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        prompt_mapping = {
            'happy': "a simple happy emoticon with a big smile",
            'sad': "a simple sad emoticon with a frown and tear",
            'angry': "a simple angry emoticon with furrowed brows",
            'neutral': "a simple neutral emoticon with straight face",
            'fear': "a simple scared emoticon with wide eyes",
            'disgust': "a simple disgusted emoticon with wrinkled nose",
            'surprise': "a simple surprised emoticon with open mouth"
        }

        prompt = prompt_mapping.get(emotion, "a simple emoticon")

        payload = {
            "prompt": prompt,
            "n": 1,
            "size": "256x256"
        }

        with st.spinner('Generating emoticon...'):
            response = requests.post(api_url, headers=headers, data=json.dumps(payload))
            response_data = response.json()

            if 'error' in response_data:
                st.error(f"API Error: {response_data['error']['message']}")
                return None

            image_url = response_data['data'][0]['url']
            image_response = requests.get(image_url)
            if image_response.status_code == 200:
                return image_response.content
            else:
                st.error(f"Failed to download image: {image_response.status_code}")
                return None

    except Exception as e:
        st.error(f"Error generating emoticon: {e}")
        return None

def main():
    st.title("üé≠ Speech Emotion Recognition")
    st.write("Upload an MP4 audio file to detect emotion")

    with st.sidebar:
        st.header("About")
        st.info("""
        This app recognizes emotions from speech using a neural network.

        Supported emotions:
        - Happy
        - Sad
        - Angry
        - Neutral
        - Fear
        - Disgust
        - Surprise
        """)

        st.header("Instructions")
        st.write("""
        1. Upload an MP4 audio file containing speech
        2. The app will process your audio and predict the emotion
        """)

    model = load_model()
    if model is None:
        st.warning("Please run fyp_test.py to create the model before using this app.")
        return

    uploaded_file = st.file_uploader("Upload your audio file", type=["mp4"])

    if uploaded_file is not None:
        st.audio(uploaded_file, format='audio/mp4')

        with st.spinner("Converting and processing audio..."):
            # Save uploaded MP4 to temp file
            with tempfile.NamedTemporaryFile(suffix=".mp4") as temp_mp4:
                temp_mp4.write(uploaded_file.read())
                temp_mp4.flush()

                # Convert MP4 to WAV using pydub
                audio = AudioSegment.from_file(temp_mp4.name, format="mp4")
                with tempfile.NamedTemporaryFile(suffix=".wav") as temp_wav:
                    audio.export(temp_wav.name, format="wav")

                    # Extract features using your function
                    features = get_features(temp_wav.name)  # shape (6, 52)

                    # Reshape features for model input: add channel dim if needed
                    if len(features.shape) == 2:
                        features = np.expand_dims(features, axis=-1)  # (6, 52, 1)

                    # Predict and display results
                    process_features(features, model)

def process_features(features, model):
    """Process features and display emotion prediction results"""
    with st.spinner('Analyzing emotion...'):
        predictions = model.predict(features, verbose=0)
        avg_prediction = np.mean(predictions, axis=0)
        emotion_index = np.argmax(avg_prediction)

        emotion_labels = ['neutral', 'surprise', 'happy', 'fear', 'sad', 'angry', 'disgust']
        emotion = emotion_labels[emotion_index]
        confidence = float(avg_prediction[emotion_index])

    col1, col2 = st.columns(2)

    with col1:
        st.subheader("Detected Emotion")

        emotion_colors = {
            'happy': '#FFD700',  # Gold
            'sad': '#4169E1',    # Royal Blue
            'angry': '#DC143C',  # Crimson
            'neutral': '#808080', # Gray
            'fear': '#9932CC',   # Dark Orchid
            'disgust': '#228B22', # Forest Green
            'surprise': '#FF8C00' # Dark Orange
        }

        st.markdown(
            f"""
            <div style="background-color: {emotion_colors.get(emotion, '#808080')};
                      padding: 20px;
                      border-radius: 10px;
                      text-align: center;
                      color: white;
                      font-size: 24px;
                      font-weight: bold;">
                {emotion.upper()}
            </div>
            """,
            unsafe_allow_html=True
        )

        st.subheader("Confidence")
        st.progress(confidence)
        st.text(f"{confidence:.2%}")

        st.subheader("All Probabilities")
        for i, label in enumerate(emotion_labels):
            st.text(f"{label.capitalize()}: {avg_prediction[i]:.4f}")

    with col2:
        st.subheader("Emoticon")
        emoticon_data = generate_emoticon(emotion)
        if emoticon_data:
            image = Image.open(io.BytesIO(emoticon_data))
            st.image(image, caption=f"{emotion.capitalize()} Emoticon", use_column_width=True)
        else:
            emotion_emojis = {
                'happy': "üòä", 'sad': "üò¢", 'angry': "üò†",
                'neutral': "üòê", 'fear': "üò®", 'disgust': "ü§¢", 'surprise': "üò≤"
            }
            st.markdown(f"<h1 style='text-align: center; font-size: 100px;'>{emotion_emojis.get(emotion, '‚ùì')}</h1>", unsafe_allow_html=True)

if __name__ == "__main__":
    main()
