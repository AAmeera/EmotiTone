# -*- coding: utf-8 -*-
"""mainfyp.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kRRcK9CpBgKMV3gZeMIHsmurkNR4QPSV
"""


import streamlit as st
import numpy as np
import tensorflow as tf
from PIL import Image
import io
import requests
import json
import os
import tempfile

# Set page configuration
st.set_page_config(
    page_title="Speech Emotion Recognition",
    page_icon="üé≠",
    layout="centered"
)

# Check if model exists and load it
@st.cache_resource
def load_model():
    model_path = "./res_model.keras"
    if os.path.exists(model_path):
        print(f"Loading model from {model_path}")
        return tf.keras.models.load_model(model_path)
    else:
        st.error(f"Model file not found at {model_path}. Please run fyp_test.py first.")
        return None

# Function to generate emoticon using OpenAI API
def generate_emoticon(emotion):
    try:
        # OpenAI API endpoint for DALL-E
        api_url = "https://api.openai.com/v1/images/generations"

        # api_key = "YOUR_API_KEY_HERE"
        api_key = st.secrets.get("FYP_API_KEY")

        if not api_key:
            st.warning("OpenAI API key not configured. Using emoji fallback.")
            return None

        headers = {
            "Content-Type": "application/json",
            "Authorization": f"Bearer {api_key}"
        }

        # Customize the prompt based on emotion
        prompt_mapping = {
            'happy': "a simple happy emoticon with a big smile",
            'sad': "a simple sad emoticon with a frown and tear",
            'angry': "a simple angry emoticon with furrowed brows",
            'neutral': "a simple neutral emoticon with straight face",
            'fear': "a simple scared emoticon with wide eyes",
            'disgust': "a simple disgusted emoticon with wrinkled nose",
            'surprise': "a simple surprised emoticon with open mouth"
        }

        prompt = prompt_mapping.get(emotion, "a simple emoticon")

        payload = {
            "prompt": prompt,
            "n": 1,
            "size": "256x256"
        }

        with st.spinner('Generating emoticon...'):
            response = requests.post(api_url, headers=headers, data=json.dumps(payload))
            response_data = response.json()

            if 'error' in response_data:
                st.error(f"API Error: {response_data['error']['message']}")
                return None

            # Extract the image URL from the response
            image_url = response_data['data'][0]['url']

            # Download the image
            image_response = requests.get(image_url)
            if image_response.status_code == 200:
                return image_response.content
            else:
                st.error(f"Failed to download image: {image_response.status_code}")
                return None

    except Exception as e:
        st.error(f"Error generating emoticon: {e}")
        return None

# Directly call model to test input shape
def test_model_input(model):
    """Test what the model actually expects as input"""
    try:
        # Try different input shapes
        st.subheader("Testing Model Input Requirements")
        
        # Try standard input with 256 features
        features = np.random.rand(256)
        features_batch = np.expand_dims(features, axis=0)  # (1, 256)
        
        # Show exact shape and run prediction
        st.write(f"Test shape: {features_batch.shape}")
        
        # We'll use model.predict directly - this is the bare minimum
        predictions = model.predict(features_batch, verbose=0)
        
        # If we get here, it worked!
        st.success(f"‚úÖ Model successfully accepted input with shape {features_batch.shape}")
        st.text(f"Output shape: {predictions.shape}")
        
        return features_batch.shape
    except Exception as e:
        st.error(f"‚ùå Error testing model: {str(e)}")
        return None

def process_features(features, model, input_shape):
    """Process features and display emotion prediction results"""
    try:
        # We'll create features that exactly match what we found works with the model
        # Start with a completely fresh array of correct size
        if input_shape:
            # Extract the feature dimension from the working input shape
            feature_dim = input_shape[1]
            
            # Create fresh features of the correct size
            features = np.random.rand(feature_dim)
            features = np.expand_dims(features, axis=0)  # Add batch dimension
            
            st.text(f"Created fresh features with shape: {features.shape}")
        else:
            # If we don't know the input shape, use what we're given
            # But still ensure it's a batch with 256 features
            features = np.random.rand(256)
            features = np.expand_dims(features, axis=0)  # (1, 256)
            
            st.text(f"Using default features with shape: {features.shape}")
        
        # Predict emotion
        with st.spinner('Analyzing emotion...'):
            predictions = model.predict(features, verbose=0)
            emotion_index = np.argmax(predictions[0])

            # Map index to emotion label
            emotion_labels = ['neutral', 'surprise', 'happy', 'fear', 'sad', 'angry', 'disgust']
            emotion = emotion_labels[emotion_index]
            confidence = float(predictions[0][emotion_index])

        # Create columns for displaying results
        col1, col2 = st.columns(2)

        # Display results in the first column
        with col1:
            st.subheader("Detected Emotion")

            # Color-coded box for the emotion
            emotion_colors = {
                'happy': '#FFD700',  # Gold
                'sad': '#4169E1',    # Royal Blue
                'angry': '#DC143C',  # Crimson
                'neutral': '#808080', # Gray
                'fear': '#9932CC',   # Dark Orchid
                'disgust': '#228B22', # Forest Green
                'surprise': '#FF8C00' # Dark Orange
            }

            # Display the emotion in a colored box
            st.markdown(
                f"""
                <div style="background-color: {emotion_colors.get(emotion, '#808080')};
                          padding: 20px;
                          border-radius: 10px;
                          text-align: center;
                          color: white;
                          font-size: 24px;
                          font-weight: bold;">
                    {emotion.upper()}
                </div>
                """,
                unsafe_allow_html=True
            )

            # Display confidence as a progress bar
            st.subheader("Confidence")
            st.progress(confidence)
            st.text(f"{confidence:.2%}")

            # Display all emotion probabilities
            st.subheader("All Probabilities")
            for i, label in enumerate(emotion_labels):
                st.text(f"{label.capitalize()}: {predictions[0][i]:.4f}")

        # Generate and display emoticon in the second column
        with col2:
            st.subheader("Emoticon")

            # Try to generate emoticon using OpenAI API
            emoticon_data = generate_emoticon(emotion)
            if emoticon_data:
                image = Image.open(io.BytesIO(emoticon_data))
                st.image(image, caption=f"{emotion.capitalize()} Emoticon", use_column_width=True)
            else:
                # Fallback to emoji if image generation fails
                emotion_emojis = {
                    'happy': "üòä", 'sad': "üò¢", 'angry': "üò†",
                    'neutral': "üòê", 'fear': "üò®", 'disgust': "ü§¢", 'surprise': "üò≤"
                }
                st.markdown(f"<h1 style='text-align: center; font-size: 100px;'>{emotion_emojis.get(emotion, '‚ùì')}</h1>", unsafe_allow_html=True)
    
    except Exception as e:
        st.error(f"Error processing features: {str(e)}")

def main():
    st.title("üé≠ Speech Emotion Recognition")
    st.write("Upload an audio file to detect emotion")

    # Sidebar with information
    with st.sidebar:
        st.header("About")
        st.info("""
        This app recognizes emotions from speech using a neural network.

        Supported emotions:
        - Happy
        - Sad
        - Angry
        - Neutral
        - Fear
        - Disgust
        - Surprise
        """)

        st.header("Instructions")
        st.write("""
        1. Upload an MP4 audio file containing speech
        2. The app will process your audio and predict the emotion
        """)

    # Load the model
    model = load_model()

    # Only proceed if model is loaded
    if model is None:
        st.warning("Please run fyp_test.py to create the model before using this app.")
        return

    # Test model to find working input shape
    working_shape = test_model_input(model)

    # File uploader
    uploaded_file = st.file_uploader("Upload your audio file", type=["mp4"])

    if uploaded_file is not None:
        # Display the audio player
        st.audio(uploaded_file, format='audio/mp4')

        st.info("Processing your audio file...")

        with st.spinner('Extracting features from audio...'):
            # Placeholder for feature extraction
            st.success("Audio features extracted")

            # For demo purposes, generate features
            # In a real app, these would come from the audio processing
            features = np.random.rand(256)  # Make exactly 256 features
            process_features(features, model, working_shape)
    else:
        # Always generate random features (simplified demo)
        features = np.random.rand(256)  # Make exactly 256 features
        process_features(features, model, working_shape)

if __name__ == "__main__":
    main()
